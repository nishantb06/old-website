<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Nishant Bhansali</title>
    <link>https://nishantb06.github.io/posts/</link>
    <description>Recent content in Posts on Nishant Bhansali</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Sep 2022 21:30:05 +0530</lastBuildDate><atom:link href="https://nishantb06.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mobile-VIT [Paper Summary]</title>
      <link>https://nishantb06.github.io/posts/second/</link>
      <pubDate>Fri, 09 Sep 2022 21:30:05 +0530</pubDate>
      
      <guid>https://nishantb06.github.io/posts/second/</guid>
      <description>Papers With Code
Observations Theres a global inductive bias in CNN’s (invariance to shift and scale) which is why CNN’s have comparable performance w.r.t Transformers (Reference to this statement is in the Transformer survey paper). Transformer models overcome this with the help of extensive training regimes, large datasets and larger models. (It will be good if we mention this in the paper somewhere) CoreML library was used to perform testing on I- phone 12 Good things about the paper the paper has two significant contributions A novel architecture which combines convolution block from MobileNetV2 and the self attention block.</description>
    </item>
    
  </channel>
</rss>
