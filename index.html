<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.102.3" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Nishant Bhansali</title>
  <meta name="description" content="Currently resolving CUDA out of memory error" />

  
  <link type="text/css" rel="stylesheet" href="https://nishantb06.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://nishantb06.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://nishantb06.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://nishantb06.github.io/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://nishantb06.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Nishant Bhansali" />
  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://nishantb06.github.io/"><h1>Nishant Bhansali</h1></a>
      <p class="lead">
       Currently resolving CUDA out of memory error 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://nishantb06.github.io/">Home</a> </li>
        <li><a href="/about/"> About Me </a></li><li><a href="https://github.com/nishantb06/"> Github </a></li><li><a href="https://www.kaggle.com/nishantbhansali"> Kaggle </a></li><li><a href="https://www.linkedin.com/in/nishantbhansali/"> LinkedIn </a></li><li><a href="https://twitter.com/itsnishant14"> Twitter </a></li>
      </ul>
    </nav>

    <p>&copy; 2023. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://nishantb06.github.io/posts/llama_alpaca/">The Annotated LLaMA</a>
  </h1>
  <time datetime="2023-04-15T13:34:45&#43;0530" class="post-date">Sat, Apr 15, 2023</time>
  Foreword Welcome to “The Annotated LLaMA”.
One of the most brilliant and well-explained articles I have read is The Annotated Transformer and the Annotated DETR. It introduced Attention like no other post. The simple idea was to present an “annotated” version of the paper Attention is all you need along with code.
I have tried to do something similar with the LLaMA models without which the commercial use of many Large Language models would not have been possible.
  
  <div class="read-more-link">
    <a href="/posts/llama_alpaca/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://nishantb06.github.io/posts/sscd/">A Self-Supervised Descriptor for Image Copy Detection - Review</a>
  </h1>
  <time datetime="2023-01-13T07:40:39&#43;0530" class="post-date">Fri, Jan 13, 2023</time>
  A Self-Supervised Descriptor for Image Copy Detection - Review [Paper][Code]
They have built upon the work of SimCLR and successfully tackled its limitations. Do give this paper a read if you are looking for a way of generating powerful embeddings/descriptors for your image dataset.
Good things about the paper It Introduces regularisation term based on Entropy which is used to make the descriptors more sparse. Which means that negative images wont be as “close” to each other as they used to be in SimCLR.
  
  <div class="read-more-link">
    <a href="/posts/sscd/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://nishantb06.github.io/posts/tmux/">TMUX for Machine Learning Engineers</a>
  </h1>
  <time datetime="2022-10-16T00:06:25&#43;0530" class="post-date">Sun, Oct 16, 2022</time>
  What is Tmux TMUX (Terminal Multiplexer) is a program which helps create and manage various terminal sessions created from a terminal itself. We can detach these newly created terminal which helps in asyncronously running multiple programs.
These terminal will keep on executing a particular command in the background untill we explicitly stop it after attaching it to an active terminal session.
We can create multiple terminal sessions and view and manage them in the same window by toggling between them.
  
  <div class="read-more-link">
    <a href="/posts/tmux/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://nishantb06.github.io/posts/third/">Docker Cheatsheet</a>
  </h1>
  <time datetime="2022-09-11T10:15:51&#43;0530" class="post-date">Sun, Sep 11, 2022</time>
  But it works on my machine !!?? The above sentence is exactly the problem docker solves -
Earlier there was no way to run 2 applications (different OS) on the same machine. VMware solved this problem by introducing Virtual Machines. But we would have to separately assign RAM and storage for our second machine. This was still a bottleneck as we can&rsquo;t ship applications effectively with this, which is why Docker was invented.
  
  <div class="read-more-link">
    <a href="/posts/third/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://nishantb06.github.io/posts/second/">Mobile-VIT [Paper Summary]</a>
  </h1>
  <time datetime="2022-09-09T21:30:05&#43;0530" class="post-date">Fri, Sep 9, 2022</time>
  Papers With Code
Observations Theres a global inductive bias in CNN’s (invariance to shift and scale) which is why CNN’s have comparable performance w.r.t Transformers (Reference to this statement is in the Transformer survey paper). Transformer models overcome this with the help of extensive training regimes, large datasets and larger models. (It will be good if we mention this in the paper somewhere) CoreML library was used to perform testing on I- phone 12 Good things about the paper the paper has two significant contributions A novel architecture which combines convolution block from MobileNetV2 and the self attention block.
  
  <div class="read-more-link">
    <a href="/posts/second/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://nishantb06.github.io/about/">About Me</a>
  </h1>
  <time datetime="2021-10-31T21:25:02&#43;0100" class="post-date">Sun, Oct 31, 2021</time>
  Who am I? Hi, my name is Nishant Bhansali. I&rsquo;m working as a Machine Learning Engineer at Sharechat,working remotely from Ahmedabad,India. I have majorly worked on solving Computer Vision and Digital Image Processing based problems, ands thats where I would say my expertise lies. Be it recent transformer architectures or archaic Image processing algorithms, I have my my hands dirtied by almost everything vision based. My work at Sharechat has been around Image Enhancement and Image Quality assessement.
  
  <div class="read-more-link">
    <a href="/about/">Read More…</a>
  </div>
  
</article>
</div>
    </main>

    
      
    
  </body>
</html>
